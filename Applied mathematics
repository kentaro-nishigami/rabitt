応用数学

第1章：線形代数
  まずは固有値・固有ベクトルの求め方を確認して、固有ベクトル分解、特異値・特異ベクトルの概要を知る。特異値分解の概要を知る。
  これらの基本を身に着ける。
  そもそもなぜ機械学習に行列が必要かというと、機械学習では多くの連立方程式を扱う事になるが、これを効率よく扱えるようにしたものが行列である。
  解く事自体はただの作業なのでやり方を覚えればよいが、行列を扱う上での一番のポイントは積が可変でない事である。積の順番を変えれば答えが変わる。
  この辺りに注意しながら逆行列（わり算のようなもの）を求めていく事が必要になる。ある行列Aに対してスカラーλを用いてＡx=λxにかけるとき、
  xとその係数λを行列Aに対する固有ベクトル、固有値という。これを利用して行列の計算をしていくことになる。正方形の行列を３つの行列の積に変換することを固有値分解という。
  これを利用して画像データの行列から成分の小さい部分を取り除いていくと画像がぼやけていく。つまりデータ量を減らすことができる。これらの計算はn乗を求めるときにも用いられる。

第2章：確率・統計
  条件付き確率、ベイズ則の概要を知る。期待値・分散の求め方を確認する、様々な確率分布の概要を知ることを目的とする。
  条件付確率は数式で書くと複雑だが、簡単に言えばある条件下である事象が起こる確率である。例えば雨が降っている条件下で交通事故に会う確率である。
  意味を理解すれば計算することは容易である。全事象における確率ではなく、ある事象下を全事象であるかのように考え、その事象下における確率を通常の
  手法において求める事となる。
  ベイズ則とはP(X)P(Y∣X)=P(Y)P(X∣Y)=P(X∩Y)という性質であり、意味自体は確率がわかれば簡単である。ざっくりまとめればP(X∩Y)を2通りの方法で表したもの。
  これを用いる事で、確率や確率分布の推定が非常に効率的に行えるようになる。
  期待値は確率変数に確率を掛けたもので、例えばギャンブルをするときに貰えるお金とその確率がわかっていれば、1回あたりいくら勝てるかが計算できる。
  分散はデータの散らばり具合を表しており、共分散は2つのデータ系列の傾向の違いを表す。
  有名な確率分布についてまとめる。ベルヌーイ分布はコイントスのイメージであたりか外れかで分布しているような確率分布である。マルチヌーイ分布は
  サイコロを転がしたイメージでベルヌーイよりも多くの結果が扱える。二項分布はベルヌーイ分布の多試行版であり、ガウス分布は釣鐘形の連続分布となる。

第3章：情報理論
自己情報量・シャノンエントロピーの定義を確認する。KLダイバージェンス・交差エントロピーの概要を知ることを目的とする。
情報量とは発生確率が低い事がわかった時は情報量が多い、情報量は足し算で増えていくという前提に立って定義する。
あることがわかった際のその情報量を自己情報量と呼び、i(x)=−log2P(x)という形で定義される。P(x)は確率であり、確率が
小さくなるほど情報量が大きくなることが式からもわかる。対数であることは確立のかけ算が足し算になり、計算の上で非常に便利と考えられる。
シャノンエントロピーとは情報エントロピーの事で、各事象の各事象の情報量の平均である。熱力学でも出てくるような無秩序さから来ているのであろう。
KLダイバージェンスとは２つの確率分布がどの程度似ているかを表す尺度である。完全に一致していれば0になる。  
交差エントロピーは機械学習の予測の誤差として用いられる。予想が似ていれば交差エントロピーの値が低くなるので、これを低くなるようにモデルを調整
すればよいモデルが作れるという指標になる。




