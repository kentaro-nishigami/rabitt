機械学習　
　人間が認識の仕方を教えるのではなく、機械に学習の仕方を教える

・線形回帰モデル
  ある入力から出力を予想する問題を解く、つまり回帰問題を解く際に機械学習では線形回帰モデルが良く用いられる。
  教師あり学習であり、入力とm次元パラメータの線形結合を出力するモデルである。
  基本的には予想と実際のデータの誤差を最小二乗法を用いて小さくして予想するようなモデルとなっている。
  直線によるグラフの近似はエクセルなどでも簡単にできるもので、それの多次元版を機械学習でやっているというイメージ。
  ボストンによる住宅データを線形回帰モデルで分析したものが有名でデータセットが容易されているので、実際に利用して結果を求めてみた。
  今回のデータから例えば犯罪率、部屋数で予想する。犯罪発生率0.2、部屋数6と入力すると予想はmodel.predict([[0.2, 6]])> array([20.93425001])
  となり20kドルであることが予想できた。
  2変数であれば簡単にできたが、もっと正確性を上げたい場合は説明変数を増やす事などの工夫があげられる。
  回帰はscikit-learnのライブラリを用いたら簡単に計算できた。表計算はPandas、計算はNumpyなど多くのライブラリがあるので、うまく使うと
  一からコードを書かなくて良いので非常に便利。

・非線形回帰モデル
  複雑な非線形構造を内蔵する現象に対して、非線形回帰モデリングを実施する。方法としては基底展開法などがあげられ、回帰関数として
  基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線形結合を利用する。未知パラメータは線形回帰モデルと同様に最小二乗法や
  最尤法によって推定する。基本期には関数が変わっただけで同じような仕組みで推定可能である。学習の際に注意したいのは未学習と過学習
  である。未学習はデータに対して十分小さい誤差が得られないモデルであり、誤差は減ったがテストとの差が大きいモデルを過学習という。
  学習データを増やす、不要な基底関数を削除、正則化などで防げる。正則化にはL2ノルムを利用するRidge推定量、L1ノルムを利用するLAsso推定量
  があり、これらを用いることで過学習を防ぐことができる。
  検証する際に以下のような方法があげられる。
  ホールドアウト法は有限データを学習用とテストデータに二分割し、予測精度や誤り率を推定する方法。便利だがデータが大量にないと
  正しい性能評価ができないという欠点がある。クロスバリデーション法はデータを複数に分割し、１つを検証用に用いて残りを学習というのを
  全パターンやる方法である。ホールドアウト法に比べてデータが少ない時もある程度よい評価ができる。
  実際にやってみたが、正規化パラメータが0では基底関数が増えるとかなり複雑な推定をしてしまっていて、過学習が目立った。
  正規化パラメータを0.1にすると基底関数が増えてもある程度滑らかな推定ができており過学習は起こっていないと予想される。
  こうったモデルを汎化性能が高いモデルと呼ぶ。グリッドサーチを利用すればもっともよい評価値を持つチューニングを採用してくれるはずで、
  今後実装できるモデルでは実装してみたいと思った。

・ロジスティック回帰モデル
  分類問題とはある入力からクラスに分類するクラス分類である。ロジスティック線形回帰モデルは分類問題を解くための教師あり学習であり、
  入力とm次元パラメータの線形結合をシグモイド関数に入力することで出力はy=1になる確率値になる。シグモイド関数は0~1の値を出力するので
  確率を扱う上で非常に便利である。パラメータを変える事で関数の形が変わるので多くのモデルに適した出力が得られる。シグモイド関数は、
  微分がシグモイド関数自身であらわせるeのx乗のような特性があり、尤度関数の微分を行う際にこの関数を行うと計算の面で有利である。
  最尤推定を行う際には尤度関数Ｅを最大とするパラメータを探索する事になる。ロジスティック回帰は対数尤度関数をパラメータで微分して0になる
  値を求めるが解析的には困難なので勾配降下法を用いる。これにより反復学習によって逐次的にパラメータを更新し、収束させる事が可能。
  それでも計算量が多いので確率的勾配降下法を用いる。また性能を図るためには混同行列を利用して当てはまる個数を数える。
  タイタニック号のデータを用いてロジスティック回帰モデルを作成した。そもそもデータには欠損値があったので、こちらは補完を行った。
  外れ値に対しても置換し、エラーを減らした。
  self.clf = linear_model.LogisticRegression(C=c, solver='liblinear', random_state=0)
  titanic.model(X_train, y_train)
  作成したモデルから正解率を計算すると0.72となり、そこそこ高い結果となった。回帰係数の絶対値が大きいほど影響が大きいはずなので、
  もっと色々比較してみたいと感じた。

・主成分分析
  主成分分析は多変量データを解析する方法の1つであり、変数の個数を減らす際にできるだけ情報の損失を抑える事ができる。
  小数変数を利用した分析や可視化が可能になる。係数ベクトルが変われば線形変換後の値が変化する。これは情報量を分散の大きさと
  捉える事で線形変換後の変数の分散が最大となる投影軸を探索する。この制約付き最適化問題を解く際にはラグランジュ関数を最大に
  する係数ベクトルを探索することが必要である。具体的には微分して0になる点を見つける。
  元のデータの分散共分散行列の固有値と固有ベクトルが、解となる。分散共分散行列は正定値対称行列、つまり、
  固有値は必ず0以上であり、固有ベクトルは直行する。

・アルゴリズム

・サポートベクターマシーン